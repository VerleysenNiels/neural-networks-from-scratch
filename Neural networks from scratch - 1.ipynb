{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8683732b",
   "metadata": {},
   "source": [
    "# Neural networks from scratch (part 1)\n",
    "These notebooks are me following along the course on Youtube.\n",
    "I find it a great way to refresh my understanding of neural networks.\n",
    "\n",
    "If you want to do this yourself, you can find the course here:\n",
    "\n",
    "https://www.youtube.com/watch?v=Wo5dMEP_BbI&list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711f3e17",
   "metadata": {},
   "source": [
    "## Video 1: The neuron\n",
    "\n",
    "A neuron multiplies all the inputs with its weights and then takes the sum of these elements (= dot-product). Then it adds a bias to this value to come to an output. If we keep it like this we have a linear function (output = weights * inputs + bias) and of course the neuron is very limited in what it can learn. Therefore the output of the neuron is passed through an activation function first (activations seem to be discussed more in depth in later videos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e9f9641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672ba0d4",
   "metadata": {},
   "source": [
    "Fixed implementation of what a single neuron does. (no activation function)\n",
    "In practice the neuron is some kind of object that keeps track of its weights and bias, as they need to be changed in order for the neuron to learn.\n",
    "Here we just go through the computation manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172a80aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.935"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vector = [3.2, 7.2, 0.3]\n",
    "weights = [0.63, 1.2, 0.93]\n",
    "bias = 3\n",
    "\n",
    "output = input_vector[0] * weights[0] + input_vector[1] * weights[1] + input_vector[2] * weights[2] + bias\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efbf80e",
   "metadata": {},
   "source": [
    "## Video 2: Layers\n",
    "A single neuron can only learn so much, that's why we combine multiple neurons. In the same step we call this a layer and multiple layers combined make up the neural network.\n",
    "So continuing with the previous example we can make a layer with three neurons like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0aa1c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13.935, -1.1600000000000006, -1.8399999999999994]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output of previous layer\n",
    "input_vector = [3.2, 7.2, 0.3]\n",
    "\n",
    "# Neuron 1\n",
    "weights_1 = [0.63, 1.2, 0.93]\n",
    "bias_1 = 3\n",
    "\n",
    "# Neuron 2\n",
    "weights_2 = [1.4, -0.7, 2.1]\n",
    "bias_2 = -1.23\n",
    "\n",
    "# Neuron 3\n",
    "weights_3 = [-1.2, 0.1, 3.5]\n",
    "bias_3 = 0.23\n",
    "\n",
    "# Output of this layer\n",
    "output = [\n",
    "    input_vector[0] * weights_1[0] + input_vector[1] * weights_1[1] + input_vector[2] * weights_1[2] + bias_1,\n",
    "    input_vector[0] * weights_2[0] + input_vector[1] * weights_2[1] + input_vector[2] * weights_2[2] + bias_2,\n",
    "    input_vector[0] * weights_3[0] + input_vector[1] * weights_3[1] + input_vector[2] * weights_3[2] + bias_3\n",
    "]\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804ab19b",
   "metadata": {},
   "source": [
    "## Video 3: The dot product\n",
    "Previous parts contain the basic computations of a single neuron, but it can be simplified a lot by using vectors.\n",
    "All the weights and biases of a single layer are stored together and through vector operations we can easily compute the output vector of the layer, given an input vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fdbea79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.935, -1.16 , -1.84 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We keep the same input vector as before\n",
    "input_vector = [3.2, 7.2, 0.3]\n",
    "\n",
    "# The weights of the three neurons are now combined in a single vector\n",
    "weights = [\n",
    "    [0.63, 1.2, 0.93],\n",
    "    [1.4, -0.7, 2.1],\n",
    "    [-1.2, 0.1, 3.5]\n",
    "]\n",
    "\n",
    "# The same is done with the biases\n",
    "biases = [3, -1.23, 0.23]\n",
    "\n",
    "# Now we could do the same computation as before through a for-loop, but that's not very clean\n",
    "# We can use the dot product, for instance through numpy\n",
    "output = np.dot(weights, input_vector) + biases\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf4596c",
   "metadata": {},
   "source": [
    "### Intermezzo 1: Shapes\n",
    "The shape defines the structure of the array. So a (4,) shape means that we have a vector with 4 elements, a (2, 4) shape means we have a matrix of 2 rows with 4 columns each. The shape has to be homologous (within each dimension there needs to be an equal number of elements, e.g. if we have a 2 by four shape we basically have a list with 2 lists of 4 elements each, no more no less)\n",
    "\n",
    "A tensor is an object that can be represented as an array. In deep learning frameworks we work with tensors in the array format.\n",
    "\n",
    "To come back to shapes in the previous cell we did the dot product of weights and input_vector, not the other way around.\n",
    "The case here is kind of special, weights has a shape of (3, 3) and input_vector has one of (3,). Doing it in this order will perform the dot product for each row of weights, which has a shape of (3,) as well.\n",
    "Doing it the other way around will return something, but it won't be correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03aeea08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.736, -2.4  , 19.376])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What if we turn it around?\n",
    "output = np.dot(input_vector, weights) + biases\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7657ff6",
   "metadata": {},
   "source": [
    "Basically, each row of weights was multiplied with one input and then summed up. If we remove one neuron from the example, this will no longer work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e41cbec4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,) and (2,3) not aligned: 3 (dim 0) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-306f6bdd9674>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# What if we turn it around now?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (3,) and (2,3) not aligned: 3 (dim 0) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "# We keep the same input vector as before\n",
    "input_vector = [3.2, 7.2, 0.3]\n",
    "\n",
    "# Remove the third neuron\n",
    "weights = [\n",
    "    [0.63, 1.2, 0.93],\n",
    "    [1.4, -0.7, 2.1]\n",
    "]\n",
    "\n",
    "# The same is done with the biases\n",
    "biases = [3, -1.23]\n",
    "\n",
    "# What if we turn it around now?\n",
    "output = np.dot(input_vector, weights) + biases\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f150fe37",
   "metadata": {},
   "source": [
    "The error message clearly shows the problem. If we put weights back first again, we get the correct behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94e30665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.935, -1.16 ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Puttin weights first\n",
    "output = np.dot(weights, input_vector) + biases\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8ddaf9",
   "metadata": {},
   "source": [
    "## Video 4: Batches\n",
    "In previous parts we only computed the output of one layer for a single input. However, to speed things up we can perform computations in parallel.\n",
    "By using a GPU we can do lots of these computations in parallel, really speeding up the network.\n",
    "\n",
    "Aside from the processing speed, batches are really great for another reason. When training the neural network we are trying to figure out the weights and biases of the network as to best fit the training data. Now if we take one sample (that is a single input-output combination) we can make the network perfectly fit this relation. However, when we take another sample it hasn't really improved. The network needs to learn to generalize the data and therefore needs to figure out the weights and biases that best fit all the training samples together. To do this we can change the weights and biases to better fit a batch of training data at a time. (The specifics of how the network learns comes in one of the later lessons). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86195865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 13.935,  -1.16 ,  -1.84 ],\n",
       "       [  6.789,  -4.24 ,   3.76 ],\n",
       "       [ -0.489,   5.49 ,  -3.91 ],\n",
       "       [  2.175, -10.82 ,   5.18 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We now have a batch of 4 input vectors\n",
    "input_batch = [[3.2, 7.2, 0.3],\n",
    "                [-1.2, 3.4, 0.5],\n",
    "                [2.8, -4.3, -0.1],\n",
    "                [-5.1, 2.3, -0.4]\n",
    "]\n",
    "\n",
    "# The weights of the three neurons remain exactly the same\n",
    "weights = [\n",
    "    [0.63, 1.2, 0.93],\n",
    "    [1.4, -0.7, 2.1],\n",
    "    [-1.2, 0.1, 3.5]\n",
    "]\n",
    "\n",
    "# The biases also remain the same\n",
    "biases = [3, -1.23, 0.23]\n",
    "\n",
    "# Producing the output of the layer is now a matrix multiplication of the input batch and the weights\n",
    "# Mind however that the shape needs to be correct, in either order the dot product has a shape mismatch\n",
    "# This because the second element needs to be transposed (write this down if not clear why, it's very straightforward)\n",
    "output = np.dot(input_batch, np.array(weights).T) + biases\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a5c89a",
   "metadata": {},
   "source": [
    "These are the basics of a single layer, now we can turn this into object oriented code to make things simpler for future use. (Check notebook 2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b03be9e07b1dc030a647a403c994d1f709ab3169abfdd5c15fecb89c5578e3a"
  },
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
