{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks from scratch (part 2)\n",
    "These notebooks are me following along the course on Youtube.\n",
    "I find it a great way to refresh my understanding of neural networks.\n",
    "\n",
    "If you want to do this yourself, you can find the course here:\n",
    "\n",
    "https://www.youtube.com/watch?v=Wo5dMEP_BbI&list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video 4 (continued): neural network in object oriented code \n",
    "The first notebook covered the basic mathematics behind neurons, layers and batches. Continuing on that knowledge we can turn this into object oriented code to easily create neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_Layer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # We need something like this\n",
    "        # self.weights = [[np.random.random() for _ in range(input_size)] for _ in output_size]\n",
    "        # self.biases = [np.random.random() for _ in range(output_size)]\n",
    "        # A bit cleaner:\n",
    "        self.weights = np.random.randn(input_size, output_size)\n",
    "        self.biases = np.zeros((1, output_size))\n",
    "    def forward(self, inputs):\n",
    "        return np.dot(inputs, self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.48563807,  0.87077944]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For example make a network with two layers\n",
    "layer1 = Dense_Layer(3,7)\n",
    "layer2 = Dense_Layer(7, 2)\n",
    "\n",
    "inputs = [1.1, -0.4, 3.1]\n",
    "\n",
    "x = layer1.forward(inputs)\n",
    "output = layer2.forward(x)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video 5: hidden layer activations\n",
    "Not performing an activation of the outputs of a layer (activation y=x), makes the layer a linear function. Inputs are multiplied by weights and summed up. Combining multiple layers like this, does not change this. The network itself can only learn linear functions. Of course most of the problems have nonlinear solutions and that is why we use nonlinear activation functions in the hidden layers. There are many possible functions that can be used, for instance: step function (y = 0 if x < 0 and y = 1 if x > 0), rectified linear unit or ReLU (y = 0 if x < 0 else y = x), sigmoid (y = 1 / (1 + e^-x)) and many other variants.\n",
    "\n",
    "### ReLU\n",
    "By changing the weights and bias of a neuron the activation function is changed. For instance if the weights are negative, the activation function flips around the y-axis. Changing the bias will move the activation point (point where function switches from y = 0 to y = x). By then placing more neurons after one another, there are even more possibilities. They can for instance two neurons can model a function where y = value1 if x < a, y = value 2 if x > b and y = x for a < x < b. Combining more and more neurons allows for more and more complex nonlinear functions to be modeled by the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1, 0, 3.1]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1.1, -0.4, 3.1]\n",
    "output = []\n",
    "\n",
    "for i in inputs:\n",
    "    if i > 0:\n",
    "        output.append(i)\n",
    "    else:\n",
    "        output.append(0)\n",
    "        \n",
    "    # Alternative: output.append(max(0, i))\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, inputs):\n",
    "        return np.maximum(0, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.57219852]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For example make a network with two layers with ReLU activations\n",
    "layer1 = Dense_Layer(3,7)\n",
    "layer2 = Dense_Layer(7, 2)\n",
    "relu = ReLU()\n",
    "\n",
    "inputs = [1.1, -0.4, 3.1]\n",
    "\n",
    "x = layer1.forward(inputs)\n",
    "x = relu.forward(x)\n",
    "x = layer2.forward(x)\n",
    "output = relu.forward(x)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video 6: Softmax activation\n",
    "When building a classifier we want the output to represent a distribution of the different classes. So all outputs are values between 0 and 1, and when summed up they total 1.The outputs then represent the certainty of the network that the input belongs to each class. To do this we need to perform two steps. First the inputs need to be exponentiated (y = e^x). This way the negative values are removed, while keeping the information of all the values. Then the values need to be normalized in order to bring the total sum to 1. Now we can implement this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.0041660239464334, 0.6703200460356393, 22.197951281441632]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [1.1, -0.4, 3.1]\n",
    "\n",
    "exponentiated_values = []\n",
    "\n",
    "for i in inputs:\n",
    "    exponentiated_values.append(math.e ** i)\n",
    "\n",
    "exponentiated_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.11611453467414118, 0.02590865471740153, 0.8579768106084572]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_base = sum(exponentiated_values)\n",
    "normalized_values = []\n",
    "\n",
    "for value in exponentiated_values:\n",
    "    normalized_values.append(value / normalized_base)\n",
    "\n",
    "normalized_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this can be cleaned up a bit. The two steps can easily be combined in a single function. We also need to watch out for exploding values during the exponentiation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, inputs):\n",
    "        # Prevent exploding values\n",
    "        inputs = inputs - np.max(inputs, axis=1, keepdims=True)\n",
    "        # Exponentiation\n",
    "        exponentiated_values = np.exp(inputs)\n",
    "        # Normalization\n",
    "        return exponentiated_values / np.sum(exponentiated_values, axis=1, keepdims=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b03be9e07b1dc030a647a403c994d1f709ab3169abfdd5c15fecb89c5578e3a"
  },
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
